# 4 决策树(decision tree)
## 4.1 基本流程
在决策树基本算法中，有三种情形会导致递归返回:
1. 当前结点包含的样本全属于同一类别，无需划分;
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分;
3. 当前结点包含的样本集合为空，不能划分.

## 4.2 划分选择
纯度(purity)

划分方法：
- 信息增益
- 增益率
- 基尼指数

### 4.2.1 信息增益
信息熵(information entropy)越小，纯度越高

信息增益(information gain: 
$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{\lvert D^{v}\rvert}{\lvert D \rvert}Ent(D^{v})$$
- 信息增益越大，纯度提升越大。
- ID3算法
	- **离散**
	- 存在的问题：信息增益对可取值数目较多的属性有所偏好

### 4.2.2 增益率
- C4.5
	- 不直接使用信息增益，而是使用**信息增益率**
	- 先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的.
	- 可以处理连续属性

### 4.2.3 基尼指数
- CART
	- 使用基尼指数: $$Gini(D)=\sum_{k=1}^{\lvert y \rvert}\sum_{k^\prime\ne k}p_{k}p_{k^\prime}$$
	- 选择使划分后使基尼指数最小的属性作为最优划分属性

## 4.3 剪枝处理(Pruning)
避免过拟合
- 预剪枝
- 后剪枝

## 4.4 连续与缺失值
连续属性离散化（二分法）

缺失值处理

## 4.5 多变量决策树

