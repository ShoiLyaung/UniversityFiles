# 论文题目
**第一人称视角视频的事件分割与指导生成**
Ego-Centric Video Event Segmentation and Instruction Generation

# 论文概述
第一人称视角视频（Ego-Centric Video）在生活记录、智能机器人、人机交互等领域具有重要的应用价值。然而，由于其数据复杂性和时间跨度长，如何有效地分割视频中的事件并生成针对性指导仍是一个具有挑战性的问题。本课题旨在开发一个多模态模型，能够实现以下目标：

1. **事件分割**：自动将长时间的第一人称视角视频分割为短片段，每个片段代表一个完整的动作或事件单元，准确识别事件边界。
2. **内容描述生成**：为每个片段生成简洁的描述文本，以便快速理解视频内容。
3. **任务指导生成**：基于分割后的视频内容，生成任务指导文本，不仅包含单步动作，还涵盖多步任务的执行细则和策略建议，为任务执行者提供清晰的操作流程。

该研究将结合指令微调（Instruction Fine-tuning）的方法，探索模型在复杂事件上下文理解中的潜力，并验证其在Ego4D数据集上的性能。

# 论文要求
1. 数据预处理：

   整理Ego4D等第一人称视角视频数据集，提取相关的动作片段和上下文信息。

   针对长时间视频设计数据增强策略，提升模型的泛化能力。

2. 模型开发与优化：

   设计和实现一个多模态大语言模型，结合视觉、语言和动作信息，实现事件分割和内容生成。

   在模型微调中，引入指令微调方法，提高模型对长时序事件的理解和跨事件边界识别的能力。

3. 任务生成设计：

   基于分割视频片段的内容，生成完整的任务指导文本，包括逐步说明和操作建议。

   结合Ego4D的"Action Prediction"任务，优化生成文本的逻辑性和准确性，满足实际应用需求。

# 研究方向
1. 长时间视频事件分割：设计基于视觉语言模型的事件分割策略，特别是跨事件边界的识别优化。
2. 任务级指导生成：结合GPT等生成式模型，探索如何生成逻辑清晰且操作性强的任务指导文本。
3. 指令微调优化：设计高质量的微调指令集，提升模型的泛化能力和任务完成质量。
4. 实际应用探索：验证模型在现实和游戏场景中的实际应用效果

# 关键字

第一人称视角，多模态大语言模型，指令微调，视频事件分割，任务指导生成

Ego-centric, Multimodal Large Language Model, Instruction Fine-tuning, Video Event Segmentation, Task Guidance Generation
